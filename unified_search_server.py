# unified_search_server.py
from typing import Any, List, Dict, Optional, Union, Tuple
import asyncio
import logging
import os
from datetime import datetime, timedelta
from functools import lru_cache
import json
import time
from collections import defaultdict

from fastmcp import FastMCP, Context
from scholarly import scholarly
import httpx
from cachetools import TTLCache
import aiohttp

# ë¡œê¹… ì„¤ì • (ìƒìš© ì„œë¹„ìŠ¤ ìˆ˜ì¤€)
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('unified_search.log', encoding='utf-8')
    ]
)

# FastMCP ì„œë²„ ì´ˆê¸°í™”
mcp = FastMCP("Unified Search Server ğŸ”")

# API ì‚¬ìš©ëŸ‰ ì¶”ì  (ëª¨ë‹ˆí„°ë§ìš©)
api_usage = defaultdict(int)
api_errors = defaultdict(int)

# ì„¤ì •
class SearchConfig:
    # API í‚¤ (í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì •)
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY", "")
    GOOGLE_CSE_ID = os.getenv("GOOGLE_CUSTOM_SEARCH_ENGINE_ID", "")  # Google Custom Search Engine ID (ì›¹ ê²€ìƒ‰ìš©)
    YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY", "")
    
    # Rate limiting - API ë¬´ë£Œ í• ë‹¹ëŸ‰ ê³ ë ¤
    RATE_LIMIT_DELAY = 1.0  # ì´ˆë‹¹ 1íšŒë¡œ ì œí•œ (ë” ì•ˆì „í•˜ê²Œ)
    
    # ìºì‹œ ì„¤ì •
    CACHE_TTL = 3600  # 1ì‹œê°„
    CACHE_MAX_SIZE = 100
    
    # Google Scholar Rate Limit (scholarly ë¼ì´ë¸ŒëŸ¬ë¦¬)
    SCHOLAR_RATE_LIMIT_DELAY = 2.0  # Google ScholarëŠ” ë” ì—„ê²©í•˜ê²Œ

# ìºì‹œ ì´ˆê¸°í™”
scholar_cache = TTLCache(maxsize=SearchConfig.CACHE_MAX_SIZE, ttl=SearchConfig.CACHE_TTL)
web_cache = TTLCache(maxsize=SearchConfig.CACHE_MAX_SIZE, ttl=SearchConfig.CACHE_TTL)
youtube_cache = TTLCache(maxsize=SearchConfig.CACHE_MAX_SIZE, ttl=SearchConfig.CACHE_TTL)

# Rate limiting ë°ì½”ë ˆì´í„° (ì†ŒìŠ¤ë³„ë¡œ ë‹¤ë¥¸ ì§€ì—° ì‹œê°„ ì ìš©)
def rate_limit(delay: float = None):
    def decorator(func):
        last_call = [0]
        
        async def wrapper(*args, **kwargs):
            # í•¨ìˆ˜ëª…ì— ë”°ë¼ ë‹¤ë¥¸ delay ì ìš©
            if delay is None:
                if 'scholar' in func.__name__:
                    actual_delay = SearchConfig.SCHOLAR_RATE_LIMIT_DELAY
                else:
                    actual_delay = SearchConfig.RATE_LIMIT_DELAY
            else:
                actual_delay = delay
                
            current_time = time.time()
            time_since_last_call = current_time - last_call[0]
            
            if time_since_last_call < actual_delay:
                await asyncio.sleep(actual_delay - time_since_last_call)
            
            last_call[0] = time.time()
            return await func(*args, **kwargs)
        
        return wrapper
    return decorator

# ìºì‹œ í‚¤ ìƒì„± í—¬í¼ í•¨ìˆ˜
def create_cache_key(**kwargs) -> str:
    return json.dumps(kwargs, sort_keys=True)

# Google Scholar ê²€ìƒ‰ í•¨ìˆ˜ (scholarly ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
async def search_google_scholar_internal(query: str, num_results: int = 5) -> List[Dict[str, Any]]:
    """Google Scholar ë‚´ë¶€ ê²€ìƒ‰ í•¨ìˆ˜"""
    try:
        # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€ (ì°¨ë‹¨ ëŒ€ë¹„)
        max_retries = 3
        retry_delay = 5.0
        
        for attempt in range(max_retries):
            try:
                search_query = scholarly.search_pubs(query)
                results = []
                
                for _ in range(num_results):
                    try:
                        article = await asyncio.to_thread(next, search_query)
                        result = {
                            'title': article.get('bib', {}).get('title', 'ì œëª© ì—†ìŒ'),
                            'authors': ', '.join(article.get('bib', {}).get('author', [])),
                            'abstract': article.get('bib', {}).get('abstract', 'ì´ˆë¡ ì—†ìŒ'),
                            'url': article.get('pub_url', 'URL ì—†ìŒ'),
                            'year': article.get('bib', {}).get('pub_year', 'ì—°ë„ ë¯¸ìƒ'),
                            'citations': article.get('num_citations', 0),
                            'source': 'Google Scholar'
                        }
                        results.append(result)
                    except StopIteration:
                        break
                
                return results
                
            except Exception as e:
                if "429" in str(e) or "captcha" in str(e).lower():
                    # Rate limit ë˜ëŠ” CAPTCHA ê°ì§€
                    if attempt < max_retries - 1:
                        logging.warning(f"Google Scholar ì°¨ë‹¨ ê°ì§€, {retry_delay}ì´ˆ í›„ ì¬ì‹œë„... (ì‹œë„ {attempt + 1}/{max_retries})")
                        await asyncio.sleep(retry_delay)
                        retry_delay *= 2  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        continue
                    else:
                        return [{
                            "error": "Google Scholarê°€ ì¼ì‹œì ìœ¼ë¡œ ì°¨ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                            "suggestion": "ìºì‹œëœ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ë‹¤ë¥¸ ê²€ìƒ‰ ì†ŒìŠ¤ë¥¼ ì´ìš©í•´ì£¼ì„¸ìš”."
                        }]
                else:
                    raise
                    
    except Exception as e:
        logging.error(f"Google Scholar ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return [{
            "error": f"Google Scholar ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}",
            "note": "Google ScholarëŠ” ê³µì‹ APIê°€ ì—†ì–´ ê°„í—ì ìœ¼ë¡œ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        }]

async def advanced_scholar_search_internal(
    query: str, 
    author: Optional[str] = None, 
    year_range: Optional[Tuple[int, int]] = None, 
    num_results: int = 5
) -> List[Dict[str, Any]]:
    """Google Scholar ê³ ê¸‰ ê²€ìƒ‰ ë‚´ë¶€ í•¨ìˆ˜"""
    try:
        # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
        search_terms = [query]
        if author:
            search_terms.append(f'author:"{author}"')
        
        full_query = ' '.join(search_terms)
        search_query = scholarly.search_pubs(full_query)
        
        results = []
        for _ in range(num_results * 2):  # í•„í„°ë§ì„ ìœ„í•´ ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            try:
                article = await asyncio.to_thread(next, search_query)
                
                # ì—°ë„ í•„í„°ë§
                if year_range:
                    pub_year = article.get('bib', {}).get('pub_year', '')
                    if pub_year and pub_year.isdigit():
                        year = int(pub_year)
                        if year < year_range[0] or year > year_range[1]:
                            continue
                
                result = {
                    'title': article.get('bib', {}).get('title', 'ì œëª© ì—†ìŒ'),
                    'authors': ', '.join(article.get('bib', {}).get('author', [])),
                    'abstract': article.get('bib', {}).get('abstract', 'ì´ˆë¡ ì—†ìŒ'),
                    'url': article.get('pub_url', 'URL ì—†ìŒ'),
                    'year': article.get('bib', {}).get('pub_year', 'ì—°ë„ ë¯¸ìƒ'),
                    'citations': article.get('num_citations', 0),
                    'source': 'Google Scholar'
                }
                results.append(result)
                
                if len(results) >= num_results:
                    break
                    
            except StopIteration:
                break
                
        return results
    except Exception as e:
        logging.error(f"ê³ ê¸‰ Scholar ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        raise

# Google Web Search (Custom Search API ì‚¬ìš©)
async def search_google_web_internal(
    query: str, 
    num_results: int = 10,
    language: str = "en",
    safe_search: str = "medium"
) -> List[Dict[str, Any]]:
    """Google Custom Search APIë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰ ë‚´ë¶€ í•¨ìˆ˜"""
    if not SearchConfig.GOOGLE_API_KEY or not SearchConfig.GOOGLE_CSE_ID:
        return [{
            "error": "Google ì›¹ ê²€ìƒ‰ APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GOOGLE_API_KEYì™€ GOOGLE_CUSTOM_SEARCH_ENGINE_ID í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        }]
    
    try:
        async with httpx.AsyncClient() as client:
            params = {
                'key': SearchConfig.GOOGLE_API_KEY,
                'cx': SearchConfig.GOOGLE_CSE_ID,
                'q': query,
                'num': min(num_results, 10),  # API limit
                'lr': f'lang_{language}',
                'safe': safe_search
            }
            
            response = await client.get(
                'https://www.googleapis.com/customsearch/v1',
                params=params
            )
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            for item in data.get('items', []):
                result = {
                    'title': item.get('title', 'ì œëª© ì—†ìŒ'),
                    'url': item.get('link', ''),
                    'snippet': item.get('snippet', ''),
                    'source': 'Google Web'
                }
                results.append(result)
                
            return results
            
    except Exception as e:
        logging.error(f"Google Web ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return [{"error": f"Google Web ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"}]

# YouTube ê²€ìƒ‰ (YouTube Data API ì‚¬ìš©)
async def search_youtube_internal(
    query: str,
    num_results: int = 10,
    video_duration: Optional[str] = None,  # short, medium, long
    upload_date: Optional[str] = None,    # hour, today, week, month, year
    order: str = "relevance"               # relevance, date, rating, viewCount
) -> List[Dict[str, Any]]:
    """YouTube Data API v3ë¥¼ ì‚¬ìš©í•œ ë™ì˜ìƒ ê²€ìƒ‰ ë‚´ë¶€ í•¨ìˆ˜"""
    if not SearchConfig.YOUTUBE_API_KEY:
        return [{
            "error": "YouTube API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. YOUTUBE_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
        }]
    
    try:
        async with httpx.AsyncClient() as client:
            params = {
                'part': 'snippet',
                'q': query,
                'key': SearchConfig.YOUTUBE_API_KEY,
                'maxResults': min(num_results, 50),  # API limit
                'type': 'video',
                'order': order
            }
            
            if video_duration:
                params['videoDuration'] = video_duration
                
            if upload_date:
                # upload_dateì— ë”°ë¥¸ publishedAfter ê³„ì‚°
                now = datetime.utcnow()
                if upload_date == 'hour':
                    published_after = now - timedelta(hours=1)
                elif upload_date == 'today':
                    published_after = now - timedelta(days=1)
                elif upload_date == 'week':
                    published_after = now - timedelta(weeks=1)
                elif upload_date == 'month':
                    published_after = now - timedelta(days=30)
                elif upload_date == 'year':
                    published_after = now - timedelta(days=365)
                else:
                    published_after = None
                    
                if published_after:
                    params['publishedAfter'] = published_after.isoformat() + 'Z'
            
            response = await client.get(
                'https://www.googleapis.com/youtube/v3/search',
                params=params
            )
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            for item in data.get('items', []):
                snippet = item.get('snippet', {})
                result = {
                    'title': snippet.get('title', 'ì œëª© ì—†ìŒ'),
                    'channel': snippet.get('channelTitle', 'ì•Œ ìˆ˜ ì—†ëŠ” ì±„ë„'),
                    'description': snippet.get('description', ''),
                    'url': f"https://www.youtube.com/watch?v={item['id']['videoId']}",
                    'published_at': snippet.get('publishedAt', ''),
                    'thumbnail': snippet.get('thumbnails', {}).get('medium', {}).get('url', ''),
                    'source': 'YouTube'
                }
                results.append(result)
                
            return results
            
    except Exception as e:
        logging.error(f"YouTube ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return [{"error": f"YouTube ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"}]

# MCP ë„êµ¬ ì •ì˜
@mcp.tool()
@rate_limit()  # Google ScholarëŠ” ìë™ìœ¼ë¡œ 2ì´ˆ ì§€ì—°
async def search_google_scholar(
    query: str, 
    num_results: int = 5,
    ctx: Context
) -> List[Dict[str, Any]]:
    """
    Search for academic papers on Google Scholar using keywords.
    
    Args:
        query: Search query string
        num_results: Number of results to return (default: 5)
        ctx: MCP context for logging
        
    Returns:
        List of dictionaries containing paper information
    """
    cache_key = create_cache_key(query=query, num_results=num_results)
    
    # ìºì‹œ í™•ì¸
    if cache_key in scholar_cache:
        await ctx.info(f"ìºì‹œëœ Google Scholar ê²°ê³¼ ë°˜í™˜: {query}")
        return scholar_cache[cache_key]
    
    await ctx.info(f"Google Scholar ê²€ìƒ‰ ì¤‘: {query}")
    await ctx.report_progress(10, 100)
    
    try:
        results = await search_google_scholar_internal(query, num_results)
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì  (scholarly ë¼ì´ë¸ŒëŸ¬ë¦¬)
        if results and not (isinstance(results[0], dict) and 'error' in results[0]):
            api_usage["google_scholar"] += 1
            logging.info(f"Google Scholar ê²€ìƒ‰: {api_usage['google_scholar']}íšŒ")
        
        # ê²°ê³¼ ìºì‹±
        scholar_cache[cache_key] = results
        
        await ctx.report_progress(100, 100)
        await ctx.info(f"{len(results)}ê°œì˜ Google Scholar ê²°ê³¼ ë°œê²¬")
        return results
        
    except Exception as e:
        api_errors["google_scholar"] += 1
        await ctx.error(f"Google Scholar ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return [{"error": f"Google Scholar ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"}]

@mcp.tool()
@rate_limit()  # Google ScholarëŠ” ìë™ìœ¼ë¡œ 2ì´ˆ ì§€ì—°
async def search_google_scholar_advanced(
    query: str,
    author: Optional[str] = None,
    year_start: Optional[int] = None,
    year_end: Optional[int] = None,
    num_results: int = 5,
    ctx: Context
) -> List[Dict[str, Any]]:
    """
    Advanced search for academic papers on Google Scholar with filters.
    
    Args:
        query: General search query
        author: Author name to filter by
        year_start: Start year for publication date filter
        year_end: End year for publication date filter
        num_results: Number of results to return (default: 5)
        ctx: MCP context
        
    Returns:
        List of dictionaries containing paper information
    """
    year_range = None
    if year_start and year_end:
        year_range = (year_start, year_end)
    
    cache_key = create_cache_key(
        query=query, 
        author=author, 
        year_range=year_range, 
        num_results=num_results
    )
    
    # ìºì‹œ í™•ì¸
    if cache_key in scholar_cache:
        await ctx.info(f"ìºì‹œëœ ê³ ê¸‰ Scholar ê²°ê³¼ ë°˜í™˜")
        return scholar_cache[cache_key]
    
    await ctx.info(f"ê³ ê¸‰ Google Scholar ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘")
    await ctx.report_progress(10, 100)
    
    try:
        results = await advanced_scholar_search_internal(
            query, author, year_range, num_results
        )
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì 
        if results and not (isinstance(results[0], dict) and 'error' in results[0]):
            api_usage["google_scholar"] += 1
            logging.info(f"Google Scholar ê³ ê¸‰ ê²€ìƒ‰: {api_usage['google_scholar']}íšŒ")
        
        # ê²°ê³¼ ìºì‹±
        scholar_cache[cache_key] = results
        
        await ctx.report_progress(100, 100)
        await ctx.info(f"{len(results)}ê°œì˜ ê²°ê³¼ ë°œê²¬")
        return results
        
    except Exception as e:
        api_errors["google_scholar"] += 1
        await ctx.error(f"ê³ ê¸‰ Scholar ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return [{"error": f"ê³ ê¸‰ Scholar ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"}]

@mcp.tool()
@rate_limit()
async def search_google_web(
    query: str,
    num_results: int = 10,
    language: str = "en",
    safe_search: str = "medium",
    ctx: Context
) -> List[Dict[str, Any]]:
    """
    Search the web using Google Custom Search API.
    
    Args:
        query: Search query string
        num_results: Number of results (max 10 per API limits)
        language: Language code (e.g., 'en', 'ko', 'ja')
        safe_search: Safe search level ('high', 'medium', 'off')
        ctx: MCP context
        
    Returns:
        List of search results with title, URL, and snippet
    """
    cache_key = create_cache_key(
        query=query, 
        num_results=num_results, 
        language=language, 
        safe_search=safe_search
    )
    
    # ìºì‹œ í™•ì¸
    if cache_key in web_cache:
        await ctx.info(f"ìºì‹œëœ ì›¹ ê²°ê³¼ ë°˜í™˜: {query}")
        return web_cache[cache_key]
    
    await ctx.info(f"Google Web ê²€ìƒ‰ ì¤‘: {query}")
    await ctx.report_progress(10, 100)
    
    try:
        results = await search_google_web_internal(query, num_results, language, safe_search)
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì 
        if results and not (isinstance(results[0], dict) and 'error' in results[0]):
            api_usage["google_web"] += 1
            logging.info(f"Google Web API ì‚¬ìš©: {api_usage['google_web']}/100")
        
        # ì„±ê³µì ì¸ ê²½ìš° ìºì‹±
        if results and not (isinstance(results[0], dict) and 'error' in results[0]):
            web_cache[cache_key] = results
    
        await ctx.report_progress(100, 100)
        await ctx.info(f"{len(results)}ê°œì˜ ì›¹ ê²°ê³¼ ë°œê²¬")
        return results
    except Exception as e:
        api_errors["google_web"] += 1
        raise

@mcp.tool()
@rate_limit()
async def search_youtube(
    query: str,
    num_results: int = 10,
    video_duration: Optional[str] = None,
    upload_date: Optional[str] = None,
    order: str = "relevance",
    ctx: Context
) -> List[Dict[str, Any]]:
    """
    Search for videos on YouTube using YouTube Data API.
    
    Args:
        query: Search query string
        num_results: Number of results (max 50)
        video_duration: Duration filter ('short', 'medium', 'long')
        upload_date: Upload date filter ('hour', 'today', 'week', 'month', 'year')
        order: Sort order ('relevance', 'date', 'rating', 'viewCount')
        ctx: MCP context
        
    Returns:
        List of video information including title, URL, channel, description
    """
    cache_key = create_cache_key(
        query=query,
        num_results=num_results,
        video_duration=video_duration,
        upload_date=upload_date,
        order=order
    )
    
    # ìºì‹œ í™•ì¸
    if cache_key in youtube_cache:
        await ctx.info(f"ìºì‹œëœ YouTube ê²°ê³¼ ë°˜í™˜: {query}")
        return youtube_cache[cache_key]
    
    await ctx.info(f"YouTube ê²€ìƒ‰ ì¤‘: {query}")
    await ctx.report_progress(10, 100)
    
    try:
        results = await search_youtube_internal(
            query, num_results, video_duration, upload_date, order
        )
        
        # API ì‚¬ìš©ëŸ‰ ì¶”ì 
        if results and not (isinstance(results[0], dict) and 'error' in results[0]):
            api_usage["youtube"] += 1
            logging.info(f"YouTube API ì‚¬ìš©: {api_usage['youtube']}/100")
        
        # ì„±ê³µì ì¸ ê²½ìš° ìºì‹±
        if results and not (isinstance(results[0], dict) and 'error' in results[0]):
            youtube_cache[cache_key] = results
        
        await ctx.report_progress(100, 100)
        await ctx.info(f"{len(results)}ê°œì˜ YouTube ë™ì˜ìƒ ë°œê²¬")
        return results
    except Exception as e:
        api_errors["youtube"] += 1
        raise

@mcp.tool()
async def unified_search(
    query: str,
    sources: List[str] = ["scholar", "web", "youtube"],
    num_results_per_source: int = 5,
    ctx: Context
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Search across multiple sources simultaneously.
    
    Args:
        query: Search query string
        sources: List of sources to search ('scholar', 'web', 'youtube')
        num_results_per_source: Number of results per source
        ctx: MCP context
        
    Returns:
        Dictionary with results organized by source
    """
    await ctx.info(f"{len(sources)}ê°œ ì†ŒìŠ¤ì—ì„œ í†µí•© ê²€ìƒ‰ ìˆ˜í–‰")
    
    results = {}
    tasks = []
    
    # ê° ì†ŒìŠ¤ë³„ ì‘ì—… ìƒì„±
    if "scholar" in sources:
        tasks.append(("scholar", search_google_scholar_internal(query, num_results_per_source)))
    
    if "web" in sources:
        tasks.append(("web", search_google_web_internal(query, num_results_per_source)))
    
    if "youtube" in sources:
        tasks.append(("youtube", search_youtube_internal(query, num_results_per_source)))
    
    # ëª¨ë“  ê²€ìƒ‰ì„ ë™ì‹œì— ì‹¤í–‰
    total_tasks = len(tasks)
    completed = 0
    
    for source, task in tasks:
        try:
            await ctx.report_progress(completed * 100 // total_tasks, 100)
            results[source] = await task
            completed += 1
        except Exception as e:
            logging.error(f"{source} ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
            results[source] = [{"error": f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"}]
    
    await ctx.report_progress(100, 100)
    await ctx.info(f"í†µí•© ê²€ìƒ‰ ì™„ë£Œ: ì´ {sum(len(r) for r in results.values())}ê°œ ê²°ê³¼")
    
    return results

@mcp.tool()
async def get_author_info(author_name: str, ctx: Context) -> Dict[str, Any]:
    """
    Get detailed information about an author from Google Scholar.
    
    Args:
        author_name: Name of the author to search for
        ctx: MCP context
        
    Returns:
        Dictionary containing author information including publications
    """
    await ctx.info(f"ì €ì ì •ë³´ ì¡°íšŒ ì¤‘: {author_name}")
    
    try:
        search_query = scholarly.search_author(author_name)
        author = await asyncio.to_thread(next, search_query)
        filled_author = await asyncio.to_thread(scholarly.fill, author)
        
        # ê´€ë ¨ ì •ë³´ ì¶”ì¶œ
        author_info = {
            "name": filled_author.get("name", "N/A"),
            "affiliation": filled_author.get("affiliation", "N/A"),
            "interests": filled_author.get("interests", []),
            "citedby": filled_author.get("citedby", 0),
            "email": filled_author.get("email", "ì œê³µë˜ì§€ ì•ŠìŒ"),
            "homepage": filled_author.get("homepage", "ì œê³µë˜ì§€ ì•ŠìŒ"),
            "publications": [
                {
                    "title": pub.get("bib", {}).get("title", "N/A"),
                    "year": pub.get("bib", {}).get("pub_year", "N/A"),
                    "citations": pub.get("num_citations", 0),
                    "venue": pub.get("bib", {}).get("venue", "N/A")
                }
                for pub in filled_author.get("publications", [])[:10]  # ìƒìœ„ 10ê°œ ë…¼ë¬¸
            ],
            "source": "Google Scholar"
        }
        
        await ctx.info(f"ì €ì ì •ë³´ ì¡°íšŒ ì„±ê³µ")
        return author_info
        
    except Exception as e:
        await ctx.error(f"ì €ì ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {"error": f"ì €ì ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"}

@mcp.tool()
async def clear_cache(source: Optional[str] = None, ctx: Context) -> Dict[str, str]:
    """
    Clear the cache for a specific source or all sources.
    
    Args:
        source: Source to clear cache for ('scholar', 'web', 'youtube', or None for all)
        ctx: MCP context
        
    Returns:
        Status message
    """
    if source == "scholar" or source is None:
        scholar_cache.clear()
        await ctx.info("Google Scholar ìºì‹œ ì‚­ì œë¨")
    
    if source == "web" or source is None:
        web_cache.clear()
        await ctx.info("Google Web ìºì‹œ ì‚­ì œë¨")
    
    if source == "youtube" or source is None:
        youtube_cache.clear()
        await ctx.info("YouTube ìºì‹œ ì‚­ì œë¨")
    
    return {
        "status": "success",
        "message": f"ìºì‹œ ì‚­ì œ ì™„ë£Œ: {source if source else 'ëª¨ë“  ì†ŒìŠ¤'}"
    }

@mcp.tool()
async def get_api_usage_stats(ctx: Context) -> Dict[str, Any]:
    """
    Get API usage statistics for monitoring.
    
    Returns:
        Dictionary with usage stats and error counts
    """
    await ctx.info("API ì‚¬ìš©ëŸ‰ í†µê³„ ì¡°íšŒ ì¤‘")
    
    # í˜„ì¬ ë‚ ì§œì˜ í†µê³„ë§Œ ë°˜í™˜ (ì¼ë³„ ë¦¬ì…‹ ê°€ì •)
    today = datetime.now().strftime("%Y-%m-%d")
    
    stats = {
        "date": today,
        "usage": dict(api_usage),
        "errors": dict(api_errors),
        "cache_stats": {
            "scholar": {
                "size": len(scholar_cache),
                "max_size": SearchConfig.CACHE_MAX_SIZE
            },
            "web": {
                "size": len(web_cache),
                "max_size": SearchConfig.CACHE_MAX_SIZE
            },
            "youtube": {
                "size": len(youtube_cache),
                "max_size": SearchConfig.CACHE_MAX_SIZE
            }
        },
        "limits": {
            "google_web": {
                "free_quota": 100,
                "used": api_usage.get("google_web", 0),
                "remaining": max(0, 100 - api_usage.get("google_web", 0))
            },
            "youtube": {
                "free_quota": 100,  # 10000 units / 100 units per search
                "used": api_usage.get("youtube", 0),
                "remaining": max(0, 100 - api_usage.get("youtube", 0))
            }
        }
    }
    
    return stats

# ì„œë²„ ì‹œì‘ ë©”ì‹œì§€
@mcp.prompt()
async def startup_info() -> str:
    """Get information about the Unified Search Server configuration."""
    config_status = []
    
    if SearchConfig.GOOGLE_API_KEY and SearchConfig.GOOGLE_CSE_ID:
        config_status.append("âœ… Google Web Search: ì„¤ì •ë¨")
    else:
        config_status.append("âŒ Google Web Search: ë¯¸ì„¤ì • (GOOGLE_API_KEYì™€ GOOGLE_CUSTOM_SEARCH_ENGINE_ID ì„¤ì • í•„ìš”)")
    
    if SearchConfig.YOUTUBE_API_KEY:
        config_status.append("âœ… YouTube Search: ì„¤ì •ë¨")
    else:
        config_status.append("âŒ YouTube Search: ë¯¸ì„¤ì • (YOUTUBE_API_KEY ì„¤ì • í•„ìš”)")
    
    config_status.append("âœ… Google Scholar: ì¤€ë¹„ë¨ (scholarly ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)")
    
    return f"""
Unified Search MCP Server ğŸ”
===========================

ì„¤ì • ìƒíƒœ:
{chr(10).join(config_status)}

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
- search_google_scholar: í•™ìˆ  ë…¼ë¬¸ ê²€ìƒ‰
- search_google_scholar_advanced: í•„í„°ë¥¼ ì‚¬ìš©í•œ ê³ ê¸‰ í•™ìˆ  ê²€ìƒ‰
- search_google_web: ì›¹ ê²€ìƒ‰ (API í‚¤ í•„ìš”)
- search_youtube: YouTube ë™ì˜ìƒ ê²€ìƒ‰ (API í‚¤ í•„ìš”)
- unified_search: ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë™ì‹œ ê²€ìƒ‰
- get_author_info: Google Scholarì—ì„œ ì €ì ì •ë³´ ì¡°íšŒ
- clear_cache: ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì‚­ì œ
- get_api_usage_stats: API ì‚¬ìš©ëŸ‰ ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§

ìºì‹œ ì„¤ì •:
- TTL: {SearchConfig.CACHE_TTL}ì´ˆ
- ìµœëŒ€ í¬ê¸°: ì†ŒìŠ¤ë‹¹ {SearchConfig.CACHE_MAX_SIZE}ê°œ í•­ëª©
"""

if __name__ == "__main__":
    import sys
    import os
    
    # Smithery í™˜ê²½ì—ì„œëŠ” HTTP ëª¨ë“œë¡œ ì‹¤í–‰
    if os.environ.get("SMITHERY_ENV") or "--transport" in sys.argv:
        mcp.run(transport="http", port=8000)
    else:
        # ë¡œì»¬ ê°œë°œì‹œ stdio ëª¨ë“œ
        mcp.run()
